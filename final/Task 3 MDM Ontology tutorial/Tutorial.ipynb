{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Supporting data modelling through NLP</center>                         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Knowledge Modelling and Artificial Intelligence\n",
    "\n",
    "Data models and data catalogues support AI applications by providing pointers to the data needed to fuel AI applications. \n",
    "\n",
    "Likewise, AI applications in the range of Natural Language Processing can be used to support the process of data modelling. One area that gains traction from applying natural language processing on unstructured data is the area of knowledge graphs, also called ontologies. They represent a set of concepts within a domain and the relationships among those concepts. Ontologies can be used to boost AI applications, because they contain a formal expression of knowledge and can thus steer reasoning, i.e. in a chatbot.\n",
    "\n",
    "It is often difficult to find a starting point and get a complete picture of what to model in an ontology. While some approaches to tackle this problem, like posing competency questions, do not really fly in the corporate world, NLP can provide a starting point by taking various texts from the business and creating the topics and potentially reveal some connections that have not been thought of before (i.e. from wordclouds). In that sense, NLP supports speeding up the ontology creation process in an environment where a lot of written documentation is present, but overview and structure is lacking. We find this the case for many companies, where a lot of documentation about processes etc. is spread across Confluence, Jira and other tools.\n",
    "\n",
    "In this notebook, we give you an overview of the necessary steps to extract knowledge from unstructured text and pinpoint where support from data managers is needed.\n",
    "\n",
    "![title](archi1.PNG)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The architecture below summarizes the steps that are applied on text data in order to generate ontology structures. Mind that while a great deal can be automated using NLP pipelines, human experts still need to be involved in evaluating the intermediary outputs of the NLP components and formalization of ontologies. \n",
    "\n",
    "A great deal of work has been put into NLP engines by Open Source communities. And while there are a few tools out there to complete the process steps below, we have decided to make use of Stanford Core NLP, mainly because of its ability to generate triples, a common building block of ontology creation.\n",
    "\n",
    "![title](archi.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "Data preparation in this context translates to extracting data from the web and bringing it into shape for consumption by the Stanford CoreNLP engine. We do not want to go into detail with reagrds to data cleaning too much, since this is very much reliant on the underlying text data. Instead, we want to give an overview of how to properly setup all the various components of Stanford CoreNLP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup and use Stanford CoreNLP Server with Python \n",
    "\n",
    "- To download Stanford CoreNLP, go to https://stanfordnlp.github.io/CoreNLP/index.html#download and click on “Download CoreNLP”. The latest version of Stanford CoreNLP at the time of writing is v3.9.2.<br>\n",
    "\n",
    "- Once the download has completed, unzip the file using the following command: unzip stanford-corenlp-full-2017-06-09.zip <br>\n",
    "\n",
    "- Install Java 8 (if not installed) <br>\n",
    "\n",
    "- Running Stanford CoreNLP Server- Now, we have our environment ready to fire up Stanford CoreNLP Server. To do so, go to the path of the unzipped Stanford CoreNLP and execute the below command: <br>\n",
    "\n",
    " <font color='grey'> java -mx4g -cp \"*\" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -annotators \"tokenize,ssplit,pos,lemma,parse,sentiment\" -port 9000 -timeout 30000 </font> \n",
    " \n",
    " for more information, visit i.e. https://www.khalidalnajjar.com/setup-use-stanford-corenlp-server-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install all libraries and their dependencies\n",
    "\n",
    "from stanfordcorenlp import StanfordCoreNLP\n",
    "import logging\n",
    "import json\n",
    "import pandas as pd\n",
    "from nltk.parse.stanford import StanfordDependencyParser\n",
    "import os\n",
    "import numpy as np\n",
    "from graphviz import Source\n",
    "from nltk.tree import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The following definitions of class StanfordNLP are used to execute syntactic and semantic natural language processing tasks that are used for the annotation of text corpora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class StanfordNLP:\n",
    "    def __init__(self, host='http://localhost', port=9000):\n",
    "        self.nlp = StanfordCoreNLP(host, port=port,\n",
    "                                   timeout=30000)  # , quiet=False, logging_level=logging.DEBUG)\n",
    "        self.props = {\n",
    "            'annotators': 'tokenize,ssplit,pos,lemma,ner,parse,depparse,dcoref,relation',\n",
    "            'pipelineLanguage': 'en',\n",
    "            'outputFormat': 'json'\n",
    "        }\n",
    "\n",
    "    def word_tokenize(self, sentence):\n",
    "        return self.nlp.word_tokenize(sentence)\n",
    "\n",
    "    def pos(self, sentence):\n",
    "        return self.nlp.pos_tag(sentence)\n",
    "\n",
    "    def ner(self, sentence):\n",
    "        return self.nlp.ner(sentence)\n",
    "\n",
    "    def parse(self, sentence):\n",
    "        return self.nlp.parse(sentence)\n",
    "\n",
    "    def dependency_parse(self, sentence):\n",
    "        return self.nlp.dependency_parse(sentence)\n",
    "\n",
    "    def annotate(self, sentence):\n",
    "        return json.loads(self.nlp.annotate(sentence, properties=self.props))\n",
    "    \n",
    "\n",
    "    @staticmethod\n",
    "    def tokens_to_dict(_tokens):\n",
    "        tokens = defaultdict(dict)\n",
    "        for token in _tokens:\n",
    "            tokens[int(token['index'])] = {\n",
    "                'word': token['word'],\n",
    "                'lemma': token['lemma'],\n",
    "                'pos': token['pos'],\n",
    "                'ner': token['ner']\n",
    "            }\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    sNLP = StanfordNLP()\n",
    "    text= \"Rooney joined the Everton youth team at the age of 9 and made his professional debut for the club in 2002 at the age of 16.\"    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Semantic Annotation\n",
    "\n",
    "\n",
    "Any object - like a text fragment - can be examined by a multitude of annotators yielding different outputs that might be beneficial for the modelling task. For Stanford CoreNLP (https://stanfordnlp.github.io/CoreNLP/annotators.html), their github page provides a list of all their annotators and how to build custom ones. Throughout this document, we highlight the ones helping the modelling process. It depends on the texts which of the annotators yields the best results.\n",
    "\n",
    "The following steps require tinkering with the data, so some of them might need to be done again iteratively, done differently or omitted. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word Tokenization and Sentence Splitting\n",
    "\n",
    "Word tokenization is the first step of the natural language processing task which tokenizes the sentence\n",
    "into the sequence of tokens. One can already see this as an important preliminary activity to enable semantic annotation, as the number of tokens produced influences the granularity of the modelling. I.e. one could consider removing stopwords as well, but careful with that, as it may tamper with descriptions of entities or relationships if too many are omitted.\n",
    "\n",
    "Sentence splitting is often used in combination with tokenization. A missing link between the sentences could be healed by using co-variances. If many sentence start off with \"he\" or \"she\", then they should be replaced by the entity found in the previous sentence(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['Rooney', 'joined', 'the', 'Everton', 'youth', 'team', 'at', 'the', 'age', 'of', '9', 'and', 'made', 'his', 'professional', 'debut', 'for', 'the', 'club', 'in', '2002', 'at', 'the', 'age', 'of', '16', '.']\n"
     ]
    }
   ],
   "source": [
    "print (\"Tokens:\", sNLP.word_tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part of speech\n",
    "\n",
    "Part of speech tagging annotates each token with their part of speech, such as noun, verb, adjective based on its content and definition. This may already give a good approximation of the much needed subject-predicate-object triple structure required for ontology modelling, but lacks insight into the dependencies between them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS: [('Rooney', 'NNP'), ('joined', 'VBD'), ('the', 'DT'), ('Everton', 'NNP'), ('youth', 'NN'), ('team', 'NN'), ('at', 'IN'), ('the', 'DT'), ('age', 'NN'), ('of', 'IN'), ('9', 'CD'), ('and', 'CC'), ('made', 'VBD'), ('his', 'PRP$'), ('professional', 'JJ'), ('debut', 'NN'), ('for', 'IN'), ('the', 'DT'), ('club', 'NN'), ('in', 'IN'), ('2002', 'CD'), ('at', 'IN'), ('the', 'DT'), ('age', 'NN'), ('of', 'IN'), ('16', 'CD'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "print (\"POS:\", sNLP.pos(text))\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Name Entity Recognition\n",
    "\n",
    "\n",
    "Named Entity (NE) has a task of finding entities in a text such as a person, location, organization\n",
    "and country. It does so by cross-checking values against a controlled vocabulary and looking at the token structure. It is a very valuable functionality, because the named entities are already showing metadata. This makes it easier to model triples in the form subject - predicate - object. The first part of the sentence below can consequently be modelled as triple 'Person'--(joins)-->'Organization'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NER: [('Rooney', 'PERSON'), ('joined', 'O'), ('the', 'O'), ('Everton', 'ORGANIZATION'), ('youth', 'O'), ('team', 'O'), ('at', 'O'), ('the', 'O'), ('age', 'O'), ('of', 'O'), ('9', 'NUMBER'), ('and', 'O'), ('made', 'O'), ('his', 'O'), ('professional', 'O'), ('debut', 'O'), ('for', 'O'), ('the', 'O'), ('club', 'O'), ('in', 'O'), ('2002', 'DATE'), ('at', 'O'), ('the', 'O'), ('age', 'O'), ('of', 'O'), ('16', 'NUMBER'), ('.', 'O')]\n"
     ]
    }
   ],
   "source": [
    "print (\"NER:\", sNLP.ner(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shallow parsing and dependency parsing\n",
    "\n",
    "Understanding the topics and the structure of the text is instrumental for deducting the metadata and the modelling aspect of the task. Shallow parsing and dependency parsing in combination show the part of speech tags in the way they relate to each other. This helps to identify the type of relationships between the entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse: (ROOT\r\n",
      "  (S\r\n",
      "    (NP (NNP Rooney))\r\n",
      "    (VP\r\n",
      "      (VP (VBD joined)\r\n",
      "        (NP (DT the) (NNP Everton) (NN youth) (NN team))\r\n",
      "        (PP (IN at)\r\n",
      "          (NP\r\n",
      "            (NP (DT the) (NN age))\r\n",
      "            (PP (IN of)\r\n",
      "              (NP (CD 9))))))\r\n",
      "      (CC and)\r\n",
      "      (VP (VBD made)\r\n",
      "        (NP (PRP$ his) (JJ professional) (NN debut))\r\n",
      "        (PP (IN for)\r\n",
      "          (NP\r\n",
      "            (NP (DT the) (NN club))\r\n",
      "            (PP (IN in)\r\n",
      "              (NP (CD 2002)))))\r\n",
      "        (PP (IN at)\r\n",
      "          (NP\r\n",
      "            (NP (DT the) (NN age))\r\n",
      "            (PP (IN of)\r\n",
      "              (NP (CD 16)))))))\r\n",
      "    (. .)))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print (\"Parse:\", sNLP.parse(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dep Parse: [('ROOT', 0, 2), ('nsubj', 2, 1), ('det', 6, 3), ('compound', 6, 4), ('compound', 6, 5), ('dobj', 2, 6), ('case', 9, 7), ('det', 9, 8), ('nmod', 2, 9), ('case', 11, 10), ('nmod', 9, 11), ('cc', 2, 12), ('conj', 2, 13), ('nmod:poss', 16, 14), ('amod', 16, 15), ('dobj', 13, 16), ('case', 19, 17), ('det', 19, 18), ('nmod', 13, 19), ('case', 21, 20), ('nmod', 13, 21), ('case', 24, 22), ('det', 24, 23), ('nmod', 13, 24), ('case', 26, 25), ('nmod', 24, 26), ('punct', 2, 27)]\n"
     ]
    }
   ],
   "source": [
    "print (\"Dep Parse:\", sNLP.dependency_parse(text))\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ontology Creation\n",
    "\n",
    "#### Relation extraction using OPENIE CoreNLP\n",
    "\n",
    "Openie is an inbuilt functionality of the Stanford CoreNLP pipeline. It generates triples in the form s-p-o. This retrieves (multiple) individuals of a to-be ontology;  the \"data model\", the ontology itself, can be aided by extracting involved concepts, i.e. through named entity recognition, but expert knowledge is also required to decide on what is important to keep and what is not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rooney joined the Everton youth team at the age of 9 and made his professional debut for the club in 2002 at the age of 16.\n",
      "\n",
      "\n",
      "('Rooney', 'joined Everton youth team at', 'age of 9')\n",
      "('Rooney', 'made', 'his professional debut')\n",
      "('Rooney', 'made', 'his debut')\n",
      "('Rooney', 'joined Everton youth team at', 'age')\n",
      "('Rooney', 'joined', 'Everton youth team')\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from pycorenlp import *\n",
    "import collections\n",
    "nlp=StanfordCoreNLP(\"http://localhost:9000/\")\n",
    "print(text )\n",
    "print(\"\\n\")\n",
    "\n",
    "output = nlp.annotate(text, properties={\"annotators\":\"tokenize,ssplit,pos,depparse,natlog,openie\",\n",
    "                                 \"outputFormat\": \"json\",\"triple.strict\":\"true\"})\n",
    "result = [output[\"sentences\"][0][\"openie\"] for item in output]\n",
    "# print(result)\n",
    "for i in result:\n",
    "    for rel in i:\n",
    "        relationSent=rel['subject'],rel['relation'],rel['object']\n",
    "        print(relationSent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The work of experts can be facilitated through software. I.e. by presenting them different s-p-o results, they can decide whether the information contained is important. After that, the annotated entities and relationships of the remaining triples can be collected and modelled in an ontology. \n",
    "\n",
    "![title](Interface.JPG)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
